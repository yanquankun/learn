# ğŸ“˜ ä»äº†è§£åˆ°å®æˆ˜ï¼šå¤§æ¨¡å‹ Hugging Face æ¥å…¥ä¸æœ¬åœ°è°ƒè¯•å¼€å‘æŒ‡å—

## ä¸€ã€å¤§æ¨¡å‹æ¥å…¥çš„æ•´ä½“æµç¨‹

> ä½¿ç”¨å¤§æ¨¡å‹ï¼ˆå¦‚ ChatGPTã€DialoGPTã€ChatGLM ç­‰ï¼‰é€šå¸¸éµå¾ªä»¥ä¸‹æ­¥éª¤ï¼š

1. **ç¡®å®šæ¨¡å‹æº**ï¼ˆå¦‚ Hugging Faceã€OpenAIã€DeepSeek ç­‰ï¼‰
2. **åŠ è½½æ¨¡å‹**ï¼ˆé€šè¿‡ Transformers æä¾›çš„ `from_pretrained` APIï¼‰
3. **åˆå§‹åŒ–æ¨ç†é€»è¾‘**ï¼ˆä½¿ç”¨æ¨¡å‹ç±» + tokenizerï¼‰
4. **è¿›è¡Œè¾“å…¥è¾“å‡ºäº¤äº’**ï¼ˆinput â†’ model â†’ outputï¼‰
5. **è°ƒè¯• / å¾®è°ƒ / æœ¬åœ°éƒ¨ç½²**ï¼ˆæŒ‰éœ€æ±‚æ‰§è¡Œï¼‰

------

## äºŒã€ä»€ä¹ˆæ˜¯ Transformersï¼Ÿ

`Transformers` æ˜¯ç”± Hugging Face å‘å¸ƒçš„ **å¼€æº Python åº“**ï¼Œç”¨äºå¤„ç†è‡ªç„¶è¯­è¨€ï¼ˆNLPï¼‰ã€å›¾åƒï¼ˆVisionï¼‰ã€å¤šæ¨¡æ€ä»»åŠ¡ç­‰ã€‚

- âœ… å®ƒæ”¯æŒæ•°åƒç§æ¨¡å‹ï¼ˆå¦‚ GPTã€BERTã€T5ã€ChatGLM ç­‰ï¼‰
- âœ… å†…ç½®æ¨¡å‹ç±»ã€Tokenizersã€Pipeline ç­‰é«˜çº§å·¥å…·
- âœ… è‡ªåŠ¨ç®¡ç†æƒé‡ä¸‹è½½ä¸ç¼“å­˜ï¼ˆ`~/.cache/huggingface`ï¼‰

ğŸ‘‰ GitHubåœ°å€ï¼šhttps://github.com/huggingface/transformers

------

## ä¸‰ã€ä»€ä¹ˆæ˜¯ Hugging Faceï¼Ÿèƒ½åšä»€ä¹ˆï¼Ÿ

Hugging Face æ˜¯ä¸€ä¸ªä¸“æ³¨äº **äººå·¥æ™ºèƒ½æ¨¡å‹å…±äº«ã€è®­ç»ƒå’Œéƒ¨ç½²çš„å¹³å°**ï¼Œæä¾›ï¼š

- ğŸ” æ¨¡å‹åº“ï¼ˆhttps://huggingface.co/modelsï¼‰
- ğŸ“¦ æ•°æ®é›†åº“ï¼ˆhttps://huggingface.co/datasetsï¼‰
- ğŸš€ Transformers / Diffusers / PEFT ç­‰å¼€å‘åº“
- ğŸ§  æ¨ç† APIã€AutoNLPã€AutoTrainã€Space ç­‰æœåŠ¡

å®ƒæ˜¯å½“ä¸‹ä¸»æµ LLM å¼€å‘çš„ç¬¬ä¸€é€‰æ‹©ã€‚

------

## å››ã€Hugging Face æ¨¡å‹ä½¿ç”¨æ–¹å¼

### âœ… 1. åœ¨çº¿åŠ è½½ï¼ˆæ¨èåˆå­¦è€…ï¼‰

```python
from transformers import AutoModelForCausalLM, AutoTokenizer

model = AutoModelForCausalLM.from_pretrained("microsoft/DialoGPT-small")
tokenizer = AutoTokenizer.from_pretrained("microsoft/DialoGPT-small")
```

> è¿™ç§æ–¹å¼ä¼šä» `https://huggingface.co` ä¸‹è½½æ¨¡å‹ç¼“å­˜åˆ°æœ¬åœ°ã€‚

------

### âœ… 2. ç¦»çº¿ä½¿ç”¨

å¦‚æœä½ ä¸æƒ³è”ç½‘ï¼Œæ¯æ¬¡éƒ½ä»æœ¬åœ°åŠ è½½ï¼š

1. ä½¿ç”¨ CLI å·¥å…·ä¸‹è½½æ¨¡å‹ï¼š

   ```bash
   huggingface-cli login     # ç™»å½•åæ‰å¯ä¸‹è½½éƒ¨åˆ†æ¨¡å‹
   transformers-cli download microsoft/DialoGPT-small
   ```

2. æœ¬åœ°åŠ è½½æ¨¡å‹ï¼š

   ```python
   model = AutoModelForCausalLM.from_pretrained("./models/DialoGPT-small")
   ```

------

### âœ… 3. å¾®è°ƒä½¿ç”¨ï¼ˆä½œä¸ºé¢„è®­ç»ƒåŸºç¡€ï¼‰

ä½ å¯ä»¥åŸºäº DialoGPTã€ChatGLM ç­‰æ¨¡å‹ï¼Œåœ¨è‡ªå®šä¹‰å¯¹è¯æ•°æ®ä¸Šè®­ç»ƒï¼š

```python
from transformers import Trainer, TrainingArguments
# åŠ è½½ tokenizerã€modelï¼Œè‡ªå®šä¹‰ Dataset åç”¨ Trainer è®­ç»ƒ
```

------

## äº”ã€è°ƒè¯•ä¸éƒ¨ç½²å»ºè®®ï¼ˆé€‚åˆæœ¬åœ°ä½é…æœºçš„æ¨¡å‹ï¼‰

æ¨èæ¨¡å‹ï¼š

| æ¨¡å‹åç§°                           | æ¥æº | è¯´æ˜                      |
| ---------------------------------- | ---- | ------------------------- |
| `microsoft/DialoGPT-small`         | HF   | çº¦ 350MBï¼Œé€‚åˆè°ƒè¯•å¯¹è¯    |
| `THUDM/chatglm2-6b-int4`           | HF   | å·²é‡åŒ–ç‰ˆæœ¬ï¼Œé€‚é… 8GB å†…å­˜ |
| `facebook/blenderbot-400M-distill` | HF   | æ¯” DialoGPT æ›´è‡ªç„¶        |
| `tiiuae/falcon-rw-1b`              | HF   | é€‚åˆéƒ¨ç½²åˆ°è½»é‡äº‘ç¯å¢ƒ      |
| `gpt2`                             | HF   | æ ‡å‡† GPT æ¨¡å‹ï¼Œæ•™å­¦ç»å…¸   |



------

## å…­ã€ä½¿ç”¨ PyCharm æœ¬åœ°è°ƒè¯• DialoGPT æ¨¡å‹å…¨è¿‡ç¨‹

### 1. åˆ›å»º Python é¡¹ç›®ç¯å¢ƒ

- ä½¿ç”¨ PyCharm æ–°å»ºè™šæ‹Ÿç¯å¢ƒï¼ˆå»ºè®® Python 3.10+ï¼‰

- å®‰è£…ä¾èµ–ï¼š

  ```bash
  pip install transformers torch
  ```

### 2. ç¼–å†™è°ƒè¯•è„šæœ¬ï¼ˆå¦‚ `main.py`ï¼‰

```python
from transformers import AutoModelForCausalLM, AutoTokenizer
import torch

tokenizer = AutoTokenizer.from_pretrained("microsoft/DialoGPT-small")
model = AutoModelForCausalLM.from_pretrained("microsoft/DialoGPT-small")

print("ğŸ¤– è¾“å…¥ä½ çš„é—®é¢˜ï¼ˆè¾“å…¥exité€€å‡ºï¼‰ï¼š")
chat_history_ids = None

for step in range(5):
    input_text = input("ğŸ‘¤ ä½ ï¼š")
    if input_text.lower() == "exit":
        break

    new_input_ids = tokenizer.encode(input_text + tokenizer.eos_token, return_tensors='pt')
    bot_input_ids = torch.cat([chat_history_ids, new_input_ids], dim=-1) if chat_history_ids is not None else new_input_ids

    output = model.generate(bot_input_ids, max_length=1000, pad_token_id=tokenizer.eos_token_id)
    reply = tokenizer.decode(output[:, bot_input_ids.shape[-1]:][0], skip_special_tokens=True)

    print(f"ğŸ¤– AIï¼š{reply}")
    chat_history_ids = output 
```

------

## ä¸ƒã€Hugging Face æ¨¡å‹ç¦»çº¿ä¸‹è½½ã€éƒ¨ç½²ä¸æ‰“åŒ…

### 1. ç¦»çº¿ä¸‹è½½

```bash
transformers-cli login
transformers-cli download microsoft/DialoGPT-small
```

ä¸‹è½½åç»“æ„å¦‚ä¸‹ï¼š

```bash
/models/DialoGPT-small/
â”œâ”€â”€ config.json
â”œâ”€â”€ pytorch_model.bin
â”œâ”€â”€ tokenizer.json
â”œâ”€â”€ tokenizer_config.json
```

### 2. æœ¬åœ°åŠ è½½

```python
tokenizer = AutoTokenizer.from_pretrained("./models/DialoGPT-small")
model = AutoModelForCausalLM.from_pretrained("./models/DialoGPT-small")
```

### 3. æ‰“åŒ…æ–¹å¼å»ºè®®

- ç”¨ `torch.save` ä¿å­˜æƒé‡ï¼ˆä¸æ¨èï¼Œæ¨èä¿ç•™ HuggingFace ç»“æ„ï¼‰
- ä½¿ç”¨ `docker` æ„å»ºè½»é‡æ¨ç†æœåŠ¡ï¼ˆå¦‚æ­é… FastAPIï¼‰

------

## å…«ã€Cloudflare èƒ½å¦éƒ¨ç½²å¤§æ¨¡å‹ï¼Ÿ

âŒ **ä¸æ¨èåœ¨ Cloudflare Workers æˆ– Pages ä¸Šéƒ¨ç½²å¤§æ¨¡å‹**ï¼ŒåŸå› ï¼š

- CF Workers æœ€å¤§å†…å­˜çº¦ 256MBï¼Œè¿œä¸è¶³ä»¥è¿è¡Œæ¨ç†æ¨¡å‹
- æ—  GPU æ”¯æŒ
- é€‚åˆéƒ¨ç½² **API è·¯ç”±**ã€**å‰ç«¯é¡µé¢**ã€**æ¨¡å‹ä»£ç†å±‚**

âœ… æ­£ç¡®åšæ³•æ˜¯å°†æ¨¡å‹éƒ¨ç½²åœ¨ GPU Serverï¼ˆå¦‚é˜¿é‡Œäº‘ã€RunPodã€Hugging Face Inference Endpointï¼‰ï¼Œç„¶åç”¨ Cloudflare ä½œä¸ºåå‘ä»£ç†æˆ–è¾¹ç¼˜ç¼“å­˜ã€‚

------

## ä¹ã€å‚è€ƒèµ„æº

- Hugging Face å®˜ç½‘ï¼šhttps://huggingface.co
- Transformers åº“ï¼šhttps://github.com/huggingface/transformers
- DialoGPT æ¨¡å‹ä¸»é¡µï¼šhttps://huggingface.co/microsoft/DialoGPT-small
- æ¨¡å‹æœç´¢ï¼šhttps://huggingface.co/models
- å¾®è°ƒå‚è€ƒï¼šhttps://huggingface.co/blog/how-to-train